{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "55 Wide Res Net ",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/praveen460/datascience/blob/master/55_Wide_Res_Net.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "rocJJoypccQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "299474ce-3f5e-4e4b-e12b-f4e800c42e02"
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xvzf cifar-10-python.tar.gz\n",
        "!rm cifar-10-python.tar.gz\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys, os,cv2\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.misc import imread\n",
        "from scipy.misc import imresize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from skimage.transform import resize\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "def tf_relu(x): return tf.nn.relu(x)\n",
        "def d_tf_relu(x): return tf.cast(tf.greater(x,0),tf.float32)\n",
        "def tf_soft(x): return tf.nn.softmax(x)\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# data prep\n",
        "PathDicom = \"./cifar-10-batches-py/\"\n",
        "lstFilesDCM = []  # create an empty list\n",
        "for dirName, subdirList, fileList in os.walk(PathDicom):\n",
        "    for filename in fileList:\n",
        "        if not \".html\" in filename.lower() and not  \".meta\" in filename.lower():  # check whether the file's DICOM\n",
        "            lstFilesDCM.append(os.path.join(dirName,filename))\n",
        "\n",
        "# Read the data traind and Test\n",
        "batch0 = unpickle(lstFilesDCM[0])\n",
        "batch1 = unpickle(lstFilesDCM[1])\n",
        "batch2 = unpickle(lstFilesDCM[2])\n",
        "batch3 = unpickle(lstFilesDCM[3])\n",
        "batch4 = unpickle(lstFilesDCM[4])\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=True)\n",
        "train_batch = np.vstack((batch0[b'data'],batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))\n",
        "train_label = np.expand_dims(np.hstack((batch0[b'labels'],batch1[b'labels'],batch2[b'labels'],batch3[b'labels'],batch4[b'labels'])).T,axis=1).astype(np.float32)\n",
        "train_label = onehot_encoder.fit_transform(train_label).toarray().astype(np.float32)\n",
        "\n",
        "test_batch = unpickle(lstFilesDCM[5])[b'data']\n",
        "test_label = np.expand_dims(np.array(unpickle(lstFilesDCM[5])[b'labels']),axis=0).T.astype(np.float32)\n",
        "test_label = onehot_encoder.fit_transform(test_label).toarray().astype(np.float32)\n",
        "\n",
        "# reshape data\n",
        "train_batch = np.reshape(train_batch,(len(train_batch),3,32,32))\n",
        "test_batch = np.reshape(test_batch,(len(test_batch),3,32,32))\n",
        "\n",
        "# rotate data\n",
        "train_batch = np.rot90(np.rot90(train_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
        "test_batch = np.rot90(np.rot90(test_batch,1,axes=(1,3)),3,axes=(1,2)).astype(np.float32)\n",
        "\n",
        "# Normalize data from 0 to 1 per each channel\n",
        "train_batch[:,:,:,0]  = (train_batch[:,:,:,0] - train_batch[:,:,:,0].min(axis=0)) / (train_batch[:,:,:,0].max(axis=0) - train_batch[:,:,:,0].min(axis=0))\n",
        "train_batch[:,:,:,1]  = (train_batch[:,:,:,1] - train_batch[:,:,:,1].min(axis=0)) / (train_batch[:,:,:,1].max(axis=0) - train_batch[:,:,:,1].min(axis=0))\n",
        "train_batch[:,:,:,2]  = (train_batch[:,:,:,2] - train_batch[:,:,:,2].min(axis=0)) / (train_batch[:,:,:,2].max(axis=0) - train_batch[:,:,:,2].min(axis=0))\n",
        "test_batch[:,:,:,0]  = (test_batch[:,:,:,0] - test_batch[:,:,:,0].min(axis=0)) / (test_batch[:,:,:,0].max(axis=0) - test_batch[:,:,:,0].min(axis=0))\n",
        "\n",
        "# class\n",
        "class cnn0():\n",
        "    \n",
        "    def __init__(self,k,inc,out):\n",
        "        self.w1 = tf.Variable(tf.random_normal([k,k,inc,out]))\n",
        "\n",
        "    def feedforward(self,input,resadd=True):\n",
        "        self.input  = input\n",
        "        self.layer1  = tf.nn.conv2d(self.input,self.w1,strides=[1,1,1,1],padding='SAME')\n",
        "        self.layer1  = tf.nn.batch_normalization(self.layer1 ,mean=0,variance=1.0,variance_epsilon=1e-8,offset=True,scale=True)\n",
        "        self.layer1  = tf_relu(self.layer1) \n",
        "        return self.layer1 \n",
        "\n",
        "class cnn1():\n",
        "    \n",
        "    def __init__(self,k,inc,out):\n",
        "        self.w1 = tf.Variable(tf.random_normal([k,k,inc,out]))\n",
        "        self.w2 = tf.Variable(tf.random_normal([k,k,out,out]))\n",
        "        self.w3 = tf.Variable(tf.random_normal([k,k,inc,out]))\n",
        "\n",
        "    def feedforward(self,input,resadd=True):\n",
        "        self.input  = input\n",
        "\n",
        "        self.layer1  = tf.nn.conv2d(self.input,self.w1,strides=[1,1,1,1],padding='SAME')\n",
        "        self.layer1  = tf.nn.batch_normalization(self.layer1 ,mean=0,variance=1.0,variance_epsilon=1e-8,offset=True,scale=True)\n",
        "        self.layer1  = tf_relu(self.layer1) \n",
        "        self.layer1  = tf.nn.conv2d(self.layer1,self.w2,strides=[1,1,1,1],padding='SAME')\n",
        "\n",
        "        self.layer2 = tf.nn.conv2d(self.input,self.w3,strides=[1,1,1,1],padding='SAME')\n",
        "        return self.layer1 + self.layer2 \n",
        "\n",
        "class cnn2():\n",
        "    \n",
        "    def __init__(self,k,inc,out):\n",
        "        self.w1 = tf.Variable(tf.random_normal([k,k,inc,out]))\n",
        "        self.w2 = tf.Variable(tf.random_normal([k,k,out,out]))\n",
        "\n",
        "    def feedforward(self,input,resadd=True):\n",
        "        self.input  = input\n",
        "\n",
        "        self.layer1  = tf.nn.batch_normalization(self.input ,mean=0,variance=1.0,variance_epsilon=1e-8,offset=True,scale=True)\n",
        "        self.layer1  = tf_relu(self.layer1) \n",
        "        self.layer1  = tf.nn.conv2d(self.layer1,self.w1,strides=[1,1,1,1],padding='SAME')\n",
        "\n",
        "        self.layer1  = tf.nn.batch_normalization(self.layer1 ,mean=0,variance=1.0,variance_epsilon=1e-8,offset=True,scale=True)\n",
        "        self.layer1  = tf_relu(self.layer1) \n",
        "        self.layer1  = tf.nn.conv2d(self.layer1,self.w2,strides=[1,1,1,1],padding='SAME')\n",
        "        return self.layer1 + self.input \n",
        "\n",
        "# hyper\n",
        "num_epoch = 100\n",
        "batch_size = 100\n",
        "print_size = 2\n",
        "learning_rate = 0.00003\n",
        "beta1,beta2,adame = 0.9,0.999,1e-8\n",
        "\n",
        "# class\n",
        "l1_1 = cnn0(3,3,16)\n",
        "\n",
        "l2_1 = cnn1(3,16,128)\n",
        "l2_2 = cnn2(3,128,128)\n",
        "\n",
        "l3_1 = cnn1(3,128,256)\n",
        "l3_2 = cnn2(3,256,256)\n",
        "\n",
        "l4_1 = cnn1(3,256,512)\n",
        "l4_2 = cnn2(3,512,512)\n",
        "\n",
        "l5_1 = cnn0(3,512,10)\n",
        "\n",
        "# graph\n",
        "x = tf.placeholder(shape=[None,32,32,3],dtype=tf.float32)\n",
        "y = tf.placeholder(shape=[None,10],dtype=tf.float32)\n",
        "\n",
        "layer1 = l1_1.feedforward(x)\n",
        "\n",
        "layer1 = tf.nn.avg_pool(layer1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer2_1 = l2_1.feedforward(layer1)\n",
        "layer2_2 = l2_2.feedforward(layer2_1)\n",
        "\n",
        "layer3Input = tf.nn.avg_pool(layer2_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer3_1 = l3_1.feedforward(layer3Input)\n",
        "layer3_2 = l3_2.feedforward(layer3_1)\n",
        "\n",
        "layer4Input = tf.nn.avg_pool(layer3_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer4_1 = l4_1.feedforward(layer4Input)\n",
        "layer4_2 = l4_2.feedforward(layer4_1)\n",
        "\n",
        "layer5Input = tf.nn.avg_pool(layer4_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "layer5_1 = l5_1.feedforward(layer5Input)\n",
        "layer6Input = tf.nn.avg_pool(layer5_1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
        "\n",
        "final = tf.reshape(layer6Input,[batch_size,-1])\n",
        "final_soft = tf_soft(final)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=final,labels=y))\n",
        "correct_prediction = tf.equal(tf.argmax(final_soft, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# -- auto train ---\n",
        "auto_train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# session\n",
        "with tf.Session() as sess: \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    train_total_cost,train_total_acc =0,0\n",
        "    train_cost_overtime,train_acc_overtime = [],[]\n",
        "\n",
        "    test_total_cost,test_total_acc = 0,0\n",
        "    test_cost_overtime,test_acc_overtime = [],[]\n",
        "\n",
        "    # start the train\n",
        "    for iter in range(num_epoch):\n",
        "        \n",
        "        train_batch,train_label = shuffle(train_batch,train_label)\n",
        "\n",
        "        # Train Batch\n",
        "        for current_batch_index in range(0,len(train_batch),batch_size):\n",
        "            current_batch = train_batch[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_batch_label = train_label[current_batch_index:current_batch_index+batch_size,:]\n",
        "            sess_results = sess.run([cost,accuracy,correct_prediction,final_soft,final,auto_train], feed_dict= {x:current_batch,y:current_batch_label})\n",
        "            print(\"current iter:\", iter, ' current batch: ', current_batch_index, \" current cost: %.5f\"%sess_results[0],' current acc: %.5f'%sess_results[1], end='\\r')\n",
        "            train_total_cost = train_total_cost + sess_results[0]\n",
        "            train_total_acc = train_total_acc + sess_results[1]\n",
        "\n",
        "        # Test Batch\n",
        "        for current_batch_index in range(0,len(test_batch),batch_size):\n",
        "            current_batch = test_batch[current_batch_index:current_batch_index+batch_size,:,:,:]\n",
        "            current_batch_label = test_label[current_batch_index:current_batch_index+batch_size,:]\n",
        "            sess_results = sess.run([cost,accuracy,correct_prediction], feed_dict= {x:current_batch,y:current_batch_label})\n",
        "            print(\"current iter:\", iter, ' current batch: ', current_batch_index, \" current cost: %.5f\"%sess_results[0],' current acc: %.5f'%sess_results[1], end='\\r')\n",
        "            test_total_cost = test_total_cost + sess_results[0]\n",
        "            test_total_acc = test_total_acc + sess_results[1]\n",
        "\n",
        "        # store\n",
        "        train_cost_overtime.append(train_total_cost/(len(train_batch)/batch_size ) )\n",
        "        train_acc_overtime.append(train_total_acc/(len(train_batch)/batch_size ) )\n",
        "        test_cost_overtime.append(test_total_cost/(len(test_batch)/batch_size ) )\n",
        "        test_acc_overtime.append(test_total_acc/(len(test_batch)/batch_size ) )\n",
        "        \n",
        "        # print\n",
        "        if iter%print_size == 0:\n",
        "            print('\\n------ Current Iter : ',iter)\n",
        "            print(\"Avg Train Cost: \", train_cost_overtime[-1])\n",
        "            print(\"Avg Train Acc: \", train_acc_overtime[-1])\n",
        "            print(\"Avg Test Cost: \", test_cost_overtime[-1])\n",
        "            print(\"Avg Test Acc: \", test_acc_overtime[-1])\n",
        "            print('-----------')      \n",
        "        train_total_cost,train_total_acc,test_total_cost,test_total_acc=0,0,0,0            \n",
        "\n",
        "# plot and save\n",
        "plt.figure()\n",
        "plt.plot(range(len(train_cost_overtime)),train_cost_overtime,color='y',label='Original Model')\n",
        "plt.legend()\n",
        "plt.savefig('og Train Cost over time')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(len(train_acc_overtime)),train_acc_overtime,color='y',label='Original Model')\n",
        "plt.legend()\n",
        "plt.savefig('og Train Acc over time')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(len(test_cost_overtime)),test_cost_overtime,color='y',label='Original Model')\n",
        "plt.legend()\n",
        "plt.savefig('og Test Cost over time')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(len(test_acc_overtime)),test_acc_overtime,color='y',label='Original Model')\n",
        "plt.legend()\n",
        "plt.savefig('og Test Acc over time')\n",
        "\n",
        "\n",
        "\n",
        "# -- end code --"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-04-27 14:22:36--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\r\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar  92%[=================>  ] 149.90M  2.61MB/s    eta 4s     "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "cifar-10-python.tar 100%[===================>] 162.60M  2.65MB/s    in 53s     \n",
            "\n",
            "2018-04-27 14:23:29 (3.07 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "current iter: 0  current batch:  9900  current cost: 401704655659682758656.00000  current acc: 0.04000\n",
            "------ Current Iter :  0\n",
            "Avg Train Cost:  2.533686630660979e+18\n",
            "Avg Train Acc:  0.16771999985724687\n",
            "Avg Test Cost:  3.86875579983146e+20\n",
            "Avg Test Acc:  0.07609999973326921\n",
            "-----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "current iter: 2  current batch:  9900  current cost: 192924972138878730240.00000  current acc: 0.02000\n",
            "------ Current Iter :  2\n",
            "Avg Train Cost:  506778516310720.6\n",
            "Avg Train Acc:  0.10045999987050891\n",
            "Avg Test Cost:  1.79835811161037e+20\n",
            "Avg Test Acc:  0.07960000006482006\n",
            "-----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "current iter: 4  current batch:  9900  current cost: 137031843898653147136.00000  current acc: 0.05000\n",
            "------ Current Iter :  4\n",
            "Avg Train Cost:  59918288267775.17\n",
            "Avg Train Acc:  0.10028000001981854\n",
            "Avg Test Cost:  1.2703126042931606e+20\n",
            "Avg Test Acc:  0.08280000027269124\n",
            "-----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "current iter: 6  current batch:  9900  current cost: 123335412269400981504.00000  current acc: 0.06000\n",
            "------ Current Iter :  6\n",
            "Avg Train Cost:  7593834267543.96\n",
            "Avg Train Acc:  0.10032000003382563\n",
            "Avg Test Cost:  1.1465529740494846e+20\n",
            "Avg Test Acc:  0.08090000009164214\n",
            "-----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "current iter: 8  current batch:  9900  current cost: 113656217896042364928.00000  current acc: 0.05000\n",
            "------ Current Iter :  8\n",
            "Avg Train Cost:  2.302400918960571\n",
            "Avg Train Acc:  0.10033999994397164\n",
            "Avg Test Cost:  1.0568509319804492e+20\n",
            "Avg Test Acc:  0.08180000053718686\n",
            "-----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}